{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecPqMrvMLRbk"
      },
      "outputs": [],
      "source": [
        "#install packages\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load image data\n",
        "import csv\n",
        "import time\n",
        "import requests as req\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import os\n",
        "Images=[]\n",
        "list1=os.listdir(\"Picasso sample/\")\n",
        "for f1 in list1:\n",
        "  im=Image.open(\"Picasso sample/\"+f1)\n",
        "  Images.append(im)"
      ],
      "metadata": {
        "id": "l4oUeJYhLknh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import and check pytorch\n",
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "metadata": {
        "id": "s8tuq_cGLo3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize latent space embedding model\n",
        "import clip\n",
        "clip.available_models()\n",
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "sY1lifuSLzUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import relevant packages\n",
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "metadata": {
        "id": "FfM8O8CtL6Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess images\n",
        "imgs1=[]\n",
        "i=0\n",
        "for im in Images:\n",
        "  imgs1.append(preprocess(im))\n",
        "  i=i+1\n",
        "  if i%10==0:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "Lv3L_KTjL98I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Caculate and save high-dimensional embeddings\n",
        "image_input = torch.tensor(np.stack(imgs1)).cuda()\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "im_features=image_features.cpu().numpy()\n",
        "np.save(\"drive/My Drive/clip_embeddings.npy\",im_features)"
      ],
      "metadata": {
        "id": "M6iBccdjMAsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimension Reduction \n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "from sklearn import manifold\n",
        "from sklearn.metrics import euclidean_distances\n",
        "from sklearn.decomposition import PCA\n",
        "tsne = manifold.TSNE(n_components=2, init='pca', random_state=501, metric='cosine')\n",
        "pre1_t=tsne.fit_transform(im_features)\n",
        "x_min, x_max = pre1_t.min(0), pre1_t.max(0)\n",
        "pre1_norm = (pre1_t - x_min) / (x_max - x_min)"
      ],
      "metadata": {
        "id": "6rrxpuzsM6LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Dimension Reduction results \n",
        "with open('drive/My Drive/human_art/Picasso_sample.csv', 'w', newline='') as file:            \n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"path\",'x','y'])\n",
        "    for i in range(len(imgs1)):\n",
        "        writer.writerow([\"./static/Picasso sample/\"+list1[i],pre1_norm[i][0],pre1_norm[i][1]])"
      ],
      "metadata": {
        "id": "TeJ9PmwaNEso"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}